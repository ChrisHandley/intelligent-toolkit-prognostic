{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "scoring_function = [1877.612, 2030.918, 1912.785481, 1672.520841, 2050.989301, 2477.963,\n",
    "                   1540.401395, 1433.541178, 1147.4, 999.5, 890.2, 957.3,1221.4] ## Metrics\n",
    "training_time = [20.011164, 0.209155, 9.211304,37.020665,32.794138,0.029446,13.334237, \n",
    "                 1.267747,402,2625 ,262 , 1725,1051] ## SAVINGS OF A PROCESS\n",
    "r2 = np.array([0.622, 0.553,0.586,0.609,0.530, 0.464,0.621,0.559,0.34, 0.61, 0.78,  0.67, 0.72])*10\n",
    "r2 = r2.tolist()\n",
    "approach = ['SVR', 'KNN', 'XGB', 'RF', 'Adaboost', 'SGD', 'ET', 'bagging', 'LSTM', 'GRU', 'CNN_1d',\n",
    "           'CNN_2d', 'MLP']\n",
    "dd_type = ['ML','ML','ML','ML','ML','ML','ML','ML','DL','DL','DL','DL','DL']\n",
    "\n",
    "sf_vs_tt = pd.DataFrame({\"Scoring Function\": scoring_function, \"Training Time\" : training_time,\n",
    "                         \"Approach\": approach, \"R2\":r2,\"Type\": dd_type})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_vs_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#sns.scatterplot(x=\"tt\", y=\"sf\", hue=\"time\",data=sf_vs_tt_lf)\n",
    "# Load the example iris dataset\n",
    "#diamonds = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "# Draw a scatter plot while assigning point colors and sizes to different\n",
    "# variables in the dataset\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 11))\n",
    "ax = sns.scatterplot(x=\"Scoring Function\",y=\"Training Time\",hue=\"Approach\",\n",
    "                     data=sf_vs_tt,s=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "plt.figure(figsize=(10,10))\n",
    "svr = plt.scatter(1877.612, 20.011164, marker='o', color=colors[0])\n",
    "knn = plt.scatter(2030.918, 0.209155, marker='o', color=colors[1])\n",
    "xgb  = plt.scatter(1912.785481, 9.211304, marker='o', color=colors[2])\n",
    "rf  = plt.scatter(1672.520841, 37.020665, marker='o', color=colors[3])\n",
    "# dt  = plt.scatter(7.795658e+06, 0.399910, marker='o', color=colors[3])\n",
    "adb = plt.scatter(2050.989301, 32.794138, marker='o', color=colors[4])\n",
    "sgd = plt.scatter(2477.963,0.029446, marker='o', color=colors[5])\n",
    "et = plt.scatter(1540.401395,13.334237, marker='o', color=colors[6])\n",
    "bag = plt.scatter(1433.541178, 1.267747, marker='x', color=colors[7])\n",
    "\n",
    "lstm = plt.scatter(1147.4, 402, marker='x', color=colors[1])\n",
    "gru = plt.scatter(999.5,2625 , marker='x', color=colors[2])\n",
    "cnn_1d = plt.scatter(890.2, 262 , marker='x', color=colors[3])\n",
    "cnn_2d = plt.scatter(957.3, 1725, marker='x', color=colors[4])\n",
    "cnn = plt.scatter(2552.214, 2743.92, marker='x', color=colors[5])\n",
    "mlp = plt.scatter(1221.4, 1051, marker='x', color=colors[6])\n",
    "\n",
    "plt.title('Scoring Function vs Training Time')\n",
    "plt.ylabel('Time(s)')\n",
    "plt.xlabel('Scoring Function')\n",
    "\n",
    "plt.legend((svr, knn, xgb, rf, adb, sgd, et, bag, lstm, gru, cnn_1d, cnn_2d, cnn, mlp),\n",
    "           ('SVR', 'KNN', 'XGB', 'RF', 'Adaboost', 'SGD', 'ET', 'bagging', 'LSTM', 'GRU', 'CNN_1d',\n",
    "           'CNN_2d', 'CNN', 'MLP'),\n",
    "           scatterpoints=1,\n",
    "           loc='upper right',\n",
    "           ncol=3,\n",
    "           fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modelname(model):\n",
    "    name = str(model).partition('(')[0]\n",
    "    if name=='SVR':\n",
    "        name = model.get_params()['kernel'] + name\n",
    "    return(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evalModels(models, Xw, y, boxPlotOn=True):\n",
    "#     from sklearn.preprocessing import StandardScaler # Standardize data (0 mean, 1 stdev)\n",
    "#     from sklearn.model_selection import KFold\n",
    "#     from sklearn.model_selection import cross_val_score\n",
    "#     from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "#     scoring = 'neg_mean_absolute_error'  \n",
    "\n",
    "#     modelnames = []\n",
    "#     results = []\n",
    "#     for model in models:\n",
    "#         pipe = make_pipeline( StandardScaler(), model )\n",
    "#         kfold = KFold(n_splits=10, shuffle=False)\n",
    "#         cv_results = cross_val_score(pipe, Xw, y, cv=kfold, scoring=scoring)\n",
    "#         modelname = get_modelname(model)\n",
    "#         print(\"%s: %.3f %.3f\" %(modelname, -1*cv_results.mean(), cv_results.std()))\n",
    "#         modelnames.append(modelname)\n",
    "#         results.append(-1*cv_results)\n",
    "\n",
    "        \n",
    "#     if boxPlotOn:\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         # boxplot algorithm comparison\n",
    "#         fig = plt.figure()\n",
    "#         fig.suptitle('Algorithm Comparison')\n",
    "#         ax = fig.add_subplot(111)\n",
    "#         plt.boxplot(results, showmeans=True)\n",
    "#         ax.set_xticklabels(modelnames, fontsize=9)\n",
    "#         plt.xticks(rotation=90)\n",
    "#         plt.show()\n",
    "    \n",
    "#     return(results, modelnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "def prepros(train_file, test_file, ground_file):\n",
    "    # read training data - It is the aircraft engine run-to-failure data.\n",
    "    train_df = pd.read_csv(train_file, sep=\" \", header=None)\n",
    "\n",
    "    #remove the columns 26 and 27 because of NAN values\n",
    "    train_df=train_df.loc[:,0:25]\n",
    "\n",
    "    train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                         's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                         's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "    train_df = train_df.sort_values(['id','cycle'])\n",
    "\n",
    "    # read test data - It is the aircraft engine operating data without failure events recorded.\n",
    "    test_df = pd.read_csv(test_file, sep=\" \", header=None)\n",
    "\n",
    "    #remove the columns 26 and 27 because of NAN values\n",
    "    test_df=test_df.loc[:,0:25]\n",
    "\n",
    "    test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                         's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                         's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "    test_df = test_df.sort_values(['id','cycle'])\n",
    "\n",
    "    #read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
    "    truth_df = pd.read_csv(ground_file, sep=\" \", header=None)\n",
    "\n",
    "    #remove the columns 1 because of NAN values\n",
    "    truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "    truth_df.columns = ['cycle']\n",
    "\n",
    "    # Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "    ##TRAINING DATASET\n",
    "    rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "\n",
    "    train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "\n",
    "    train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "    train_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "    # generate label columns for training data\n",
    "    # we will only make use of \"label1\" for binary classification, \n",
    "    # while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
    "    w1 = 30\n",
    "    w0 = 15\n",
    "    train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "\n",
    "    train_df['label2'] = train_df['label1']\n",
    "    train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "    #check columns with repeated values: setting3, s1, s5, s10, s16,s18,s19\n",
    "    train_df.apply(lambda x: x.nunique())\n",
    "    train_df.drop(train_df.columns[[4,5,9,14,20,22,23]], axis=1, inplace=True)\n",
    "\n",
    "    #TEST DATASET\n",
    "    test_df.drop(test_df.columns[[4,5,9,14,20,22,23]], axis=1, inplace=True)\n",
    "    test_df.apply(lambda x: x.nunique())\n",
    "\n",
    "    # We use the ground truth dataset to generate labels for the test data.\n",
    "    # generate column max for test data\n",
    "    rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "\n",
    "    rul.columns = ['id', 'max']\n",
    "    truth_df.columns = ['more']\n",
    "    truth_df['id'] = truth_df.index + 1\n",
    "    truth_df['max'] = rul['max'] + truth_df['more']\n",
    "    truth_df.drop('more', axis=1, inplace=True)\n",
    "\n",
    "    # generate RUL for test data\n",
    "    test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "    test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "    test_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "    # generate label columns w0 and w1 for test data\n",
    "    test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
    "    test_df['label2'] = test_df['label1']\n",
    "    test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "    # MinMax normalization (from 0 to 1)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    ###TRAIN#####\n",
    "    train_df['cycle_norm'] = train_df['cycle']\n",
    "    cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
    "    norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                                 columns=cols_normalize, \n",
    "                                 index=train_df.index)\n",
    "\n",
    "    join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "    train_df = join_df.reindex(columns = train_df.columns)\n",
    "\n",
    "    ####TEST#####\n",
    "    test_df['cycle_norm'] = test_df['cycle']\n",
    "    test_df['id_norm'] = test_df['id']\n",
    "    norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
    "                                columns=cols_normalize, \n",
    "                                index=test_df.index)\n",
    "    test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "    test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    #### Generating input data\n",
    "    # pick the feature columns \n",
    "    sequence_cols = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
    "\n",
    "    # generate sequences and convert to numpy array\n",
    "    X_train = train_df[sequence_cols]\n",
    "\n",
    "    #obtain the last cycle data for each test battery\n",
    "    test_df=test_df.loc[test_df.groupby('id').cycle.idxmax()]\n",
    "    X_test = test_df[sequence_cols]\n",
    "    # Generate labels\n",
    "    y_train=train_df['RUL']\n",
    "    y_test=test_df['RUL']\n",
    "    return (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, y_train_1, X_test_1, y_test_1 = prepros('train_FD001.txt', 'test_FD001.txt', 'RUL_FD001.txt')\n",
    "# X_train_2, y_train_2, X_test_2, y_test_2 = prepros('train_FD002.txt', 'test_FD002.txt', 'RUL_FD002.txt')\n",
    "# X_train_3, y_train_3, X_test_3, y_test_3 = prepros('train_FD003.txt', 'test_FD003.txt', 'RUL_FD003.txt')\n",
    "# X_train_4, y_train_4, X_test_4, y_test_4 = prepros('train_FD004.txt', 'test_FD004.txt', 'RUL_FD004.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modelname(model):\n",
    "    name = str(model).partition('(')[0]\n",
    "    if name=='SVR':\n",
    "        name = model.get_params()['kernel'] + name\n",
    "    return(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_function(true, pred, err_list):\n",
    "        d = pred - true\n",
    "        length = len(d)\n",
    "        s = 0\n",
    "        for i in range(length):\n",
    "            if (d[i] < 0):\n",
    "                s += np.exp((-d[i]/10))-1\n",
    "            else:\n",
    "                s += np.exp((d[i]/13))-1\n",
    "        err_list.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(true, pred, err_list):\n",
    "    d_squared = (pred - true)**2\n",
    "    err = np.mean(d_squared)\n",
    "    err = np.sqrt(err)\n",
    "    err_list.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(true, pred, err_list):\n",
    "    d=abs(pred - true)\n",
    "    err=np.mean(d)\n",
    "    err_list.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae(true, pred, err_list):\n",
    "    d=abs(pred-true)\n",
    "    err_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re(true, pred, err_list):\n",
    "    d = (abs(pred-true)/true)\n",
    "    err_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def me(true, pred, err_list):\n",
    "    d=(sum(abs(pred-true))/len(pred))\n",
    "    err_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(true, pred, err_list):\n",
    "    err = 100/len(true) * np.sum(2 * np.abs(pred - true) / (np.abs(true) + np.abs(pred)))\n",
    "    err_list.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad(true,pred, err_list):\n",
    "    d1=(pred-true)\n",
    "    m=np.median(d1)\n",
    "    d2=np.abs(d1-m)\n",
    "    err=np.sum(d2)/len(d1)\n",
    "    err_list.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdae(true,pred, err_list):\n",
    "    d=np.abs(pred-true)\n",
    "    err=np.median(d)\n",
    "    err_list.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_results(d):\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.set_index('Model', inplace=True)\n",
    "    df.index.name='Model'\n",
    "    print(df)\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def evalModels(models, X_train, y_train, X_test, y_test, boxPlotOn=True):\n",
    "    from sklearn.preprocessing import StandardScaler # Standardize data (0 mean, 1 stdev)\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.pipeline import Pipeline, make_pipeline\n",
    "    from sklearn.metrics import r2_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    scoring = 'neg_mean_absolute_error'  \n",
    "\n",
    "    modelnames = []\n",
    "    fitted_models = []\n",
    "    results = []\n",
    "    training_time = []\n",
    "    testing_time = []\n",
    "    predicted_value = []\n",
    "    timeliness_value = []\n",
    "    rmse_value = []\n",
    "    mae_value = []\n",
    "    smape_value = []\n",
    "    r2_value = []\n",
    "    ae_value = []\n",
    "    re_value = []\n",
    "    me_value = []\n",
    "    mad_value = []\n",
    "    mdae_value = []\n",
    "    \n",
    "    def scoring_function(true, pred):\n",
    "        d = pred - true\n",
    "        length = len(d)\n",
    "        s = 0\n",
    "        for i in range(length):\n",
    "            if (d[i] < 0):\n",
    "                s += np.exp((-d[i]/10))-1\n",
    "            else:\n",
    "                s += np.exp((d[i]/13))-1\n",
    "        timeliness_value.append(s)\n",
    "        \n",
    "    def rmse(true, pred):\n",
    "        d_squared = (pred - true)**2\n",
    "        err = np.mean(d_squared)\n",
    "        err = np.sqrt(err)\n",
    "        rmse_value.append(err)\n",
    "        \n",
    "    def mae(true, pred):\n",
    "        d=abs(pred - true)\n",
    "        err=np.mean(d)\n",
    "        mae_value.append(err)\n",
    "        \n",
    "    def ae(true, pred):\n",
    "        d=abs(pred-true)\n",
    "        err=np.sum(d)\n",
    "        ae_value.append(err)\n",
    "        \n",
    "    def re(true, pred):\n",
    "        d = (abs(pred-true)/true)\n",
    "        err=np.sum(d)\n",
    "        re_value.append(err)\n",
    "        \n",
    "    def me(true, pred):\n",
    "        d=(sum(abs(pred-true))/len(pred))\n",
    "        me_value.append(d)\n",
    "        \n",
    "    def smape(true, pred):\n",
    "        err = 100/len(true) * np.sum(2 * np.abs(pred - true) / (np.abs(true) + np.abs(pred)))\n",
    "        smape_value.append(err)\n",
    "        \n",
    "    def mad(true,pred):\n",
    "        d1=(pred-true)\n",
    "        m=np.median(d1)\n",
    "        d2=np.abs(d1-m)\n",
    "        err=np.sum(d2)/len(d1)\n",
    "        mad_value.append(err)\n",
    "        \n",
    "    def mdae(true,pred):\n",
    "        d=np.abs(pred-true)\n",
    "        err=np.median(d)\n",
    "        mdae_value.append(err)\n",
    "        \n",
    "    def final_results(d):\n",
    "        df = pd.DataFrame(data=d)\n",
    "        df.set_index('Model', inplace=True)\n",
    "        df.index.name='Model'\n",
    "        print(df)\n",
    "    \n",
    "    for model in models:\n",
    "        pipe = make_pipeline( StandardScaler(), model )\n",
    "        kfold = KFold(n_splits=10, shuffle=False)\n",
    "        cv_results = cross_val_score(pipe, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        modelname = get_modelname(model)\n",
    "        print(\"%s: %.3f %.3f\" %(modelname, -1*cv_results.mean(), cv_results.std()))\n",
    "        modelnames.append(modelname)\n",
    "        results.append(-1*cv_results)\n",
    "        \n",
    "    \n",
    "    for model in models:\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        end = time.time()\n",
    "        train_time = end - start\n",
    "        fitted_models.append(model)\n",
    "        training_time.append(train_time)\n",
    "        \n",
    "    \n",
    "    for fittedmodels in fitted_models:\n",
    "        start = time.time()\n",
    "        y_pred = fittedmodels.predict(X_test)\n",
    "        end = time.time()\n",
    "        test_time = end - start\n",
    "        testing_time.append(test_time)\n",
    "        rmse(y_test.values, y_pred)\n",
    "        scoring_function(y_test.values, y_pred)\n",
    "        ae(y_test.values, y_pred)\n",
    "        re(y_test.values, y_pred)\n",
    "        me(y_test.values, y_pred)\n",
    "        mae(y_test.values, y_pred)\n",
    "        smape(y_test.values, y_pred)\n",
    "        mad(y_test.values, y_pred)\n",
    "        mdae(y_test.values, y_pred)\n",
    "        r2_value.append(r2_score(y_test.values, y_pred))\n",
    "        predicted_value.append(y_pred)\n",
    "\n",
    "        \n",
    "    if boxPlotOn:\n",
    "        fig = plt.figure()\n",
    "        palette = sns.color_palette(\"husl\")\n",
    "        sns.boxplot(data=results, palette=palette)\n",
    "        sns.set(style=\"white\",font_scale=1.13,rc={\"font.weight\": 'bold'})\n",
    "        plt.xticks(plt.xticks()[0], modelnames)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel('Mean Absolute Error')\n",
    "        fig.savefig('boxplot.png')\n",
    "        \n",
    "    data = {'Model':modelnames,\n",
    "            'Scoring Function': timeliness_value,\n",
    "            'RMSE': rmse_value,\n",
    "            'AE': ae_value,\n",
    "            'RE': re_value,\n",
    "            'ME': me_value,\n",
    "            'R^2':r2_value,\n",
    "            'MAE':mae_value,\n",
    "            'SMAPE':smape_value,\n",
    "            'MAD':mad_value,\n",
    "            'MdAE':mdae_value,\n",
    "            'Training Time': training_time,\n",
    "            'Testing Time': testing_time}\n",
    "    final_results(data)\n",
    "    return(results,\n",
    "           timeliness_value,\n",
    "           re_value,\n",
    "           me_value,\n",
    "           ae_value,\n",
    "           rmse_value,\n",
    "           mae_value,\n",
    "           smape_value,\n",
    "           r2_value,\n",
    "           mad_value,\n",
    "           mdae_value,\n",
    "           modelnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor: 34.550 9.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:   11.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:   11.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor: 26.497 8.427\n",
      "AdaBoostRegressor: 28.550 7.533\n",
      "BaggingRegressor: 28.816 8.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   14.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   14.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 728 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   14.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   15.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   14.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   17.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   13.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   17.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   16.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   20.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor: 26.425 8.436\n",
      "rbfSVR: 27.448 9.954\n",
      "XGBRegressor: 26.957 8.241\n",
      "KNeighborsRegressor: 27.550 7.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Scoring Function       RMSE           AE         RE  \\\n",
      "Model                                                                        \n",
      "SGDRegressor                2477.963462  30.402627  2520.219505  61.856006   \n",
      "ExtraTreesRegressor         1599.851794  25.911946  1927.415028  31.950798   \n",
      "AdaBoostRegressor           2016.879928  28.560149  2187.538782  42.607741   \n",
      "BaggingRegressor            1409.964418  27.188363  2228.577500  48.840779   \n",
      "RandomForestRegressor       1686.191407  25.871401  1907.284626  31.288368   \n",
      "rbfSVR                      1877.611651  25.523428  1970.694642  36.584900   \n",
      "XGBRegressor                1912.785481  26.727544  1944.877744  31.944656   \n",
      "KNeighborsRegressor         2030.918125  27.756575  2073.151378  33.135246   \n",
      "\n",
      "                              ME       R^2        MAE      SMAPE        MAD  \\\n",
      "Model                                                                         \n",
      "SGDRegressor           25.202195  0.464743  25.202195  40.650551  20.559183   \n",
      "ExtraTreesRegressor    19.274150  0.611187  19.274150  25.186995  17.602947   \n",
      "AdaBoostRegressor      21.875388  0.527653  21.875388  31.385024  19.016478   \n",
      "BaggingRegressor       22.285775  0.571938  22.285775  34.710759  17.181125   \n",
      "RandomForestRegressor  19.072846  0.612403  19.072846  24.880047  18.101409   \n",
      "rbfSVR                 19.706946  0.622759  19.706946  29.472551  17.571371   \n",
      "XGBRegressor           19.448777  0.586326  19.448777  26.009035  19.028555   \n",
      "KNeighborsRegressor    20.731514  0.553859  20.731514  26.511669  19.180982   \n",
      "\n",
      "                            MdAE  Training Time  Testing Time  \n",
      "Model                                                          \n",
      "SGDRegressor           22.204954       0.023598      0.003249  \n",
      "ExtraTreesRegressor    12.541697       5.882449      0.414429  \n",
      "AdaBoostRegressor      15.451835      31.959516      0.024569  \n",
      "BaggingRegressor       20.141250       0.665160      0.155418  \n",
      "RandomForestRegressor  12.487493      13.054894      0.207839  \n",
      "rbfSVR                 13.342231      18.276943      0.075995  \n",
      "XGBRegressor           13.963287       8.525307      0.002340  \n",
      "KNeighborsRegressor    14.580399       0.146291      0.107688  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAFjCAYAAADVUAuHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzCrIyd/MrIKc/M3MKuj/A2d5NikS5EssAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn import neighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# create and evaluate pipeline\n",
    "models = []\n",
    "models.append( SGDRegressor(shuffle=False, max_iter=None, tol=None, alpha=1.0,\n",
    "                            loss='squared_loss', penalty='l1') )\n",
    "models.append( ExtraTreesRegressor(max_features='sqrt',\n",
    "                                 min_samples_leaf=1,\n",
    "                                 min_samples_split=10,\n",
    "                                 n_estimators=700,\n",
    "                                 verbose=1,\n",
    "                                 n_jobs=-1))\n",
    "models.append( AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), \n",
    "                                 learning_rate=0.01,\n",
    "                                 loss='exponential',\n",
    "                                 n_estimators=400) )\n",
    "models.append( BaggingRegressor(n_estimators=400,\n",
    "                                bootstrap=True, \n",
    "                                max_features=18,\n",
    "                                max_samples=30,\n",
    "                                n_jobs=-1) )\n",
    "models.append( RandomForestRegressor(max_features='sqrt',\n",
    "                                     min_samples_leaf=4,\n",
    "                                     min_samples_split=10,\n",
    "                                     n_estimators=800,\n",
    "                                     verbose=1,\n",
    "                                     n_jobs=-1) )\n",
    "models.append( svm.SVR(C=1,\n",
    "                       epsilon=0.1,\n",
    "                       kernel='rbf',\n",
    "                       gamma='scale') )\n",
    "models.append( xgb.XGBRegressor(objective=\"reg:linear\",\n",
    "                                n_estimators=500,\n",
    "                                nthread=-1,\n",
    "                                silent=False,\n",
    "                                colsample_bytree=0.7,\n",
    "                                gamma=0.5,\n",
    "                                min_child_weight=1,\n",
    "                                subsample=1.0,\n",
    "                                n_jobs=-1) )\n",
    "models.append( neighbors.KNeighborsRegressor(algorithm='auto', \n",
    "                                             n_neighbors=40,\n",
    "                                             weights='distance',\n",
    "                                             n_jobs=-1) )\n",
    "\n",
    "results, timeliness,re,me,ae,rmse,mae,smape,r2,mad,mdae,modelnames = evalModels(models,X_train_1,y_train_1,X_test_1,y_test_1,boxPlotOn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25.202195048866134, 21.793474999999994, 20.731513779610626]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "sns.boxplot(data=results)\n",
    "plt.xticks(plt.xticks()[0], modelnames)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "fig.savefig('boxplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.boxplot(data=results)\n",
    "plt.xticks(plt.xticks()[0], modelnames)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = results[0:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test.append(np.array([18.01777016124075, 19.08547714234841, 18.61810288177123, 17.619403154163166, 17.469617410877248, 18.34671841387774, 17.442145677747924, 19.160146155970125, 18.430320567006447, 18.744002504926357]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames.append('CNN2D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test.append(np.array([20.998323330681167, 19.234475326514044, 18.502516721637832, 18.211856287639307, 16.800838330295846, 20.60939168987153, 20.9860453177409, 19.418558315224928, 18.131871812411365, 19.555813843401406]))\n",
    "modelnames.append('CNN1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test.append(np.array([18.661109434586884, 17.4749167644212, 16.328090963137054, 18.26126620806615, 16.439259683040785, 18.517395004384976, 17.763836526777567, 16.69404935545286, 16.896091157174286, 16.87094464406945]))\n",
    "modelnames.append('CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test.append(np.array([19.174632851347162, 14.965517455312737, 24.815246362103796, 14.69731472868515, 15.458120581318491, 26.58457925028661, 17.42879313870755, 19.904137526577433, 15.207624142549776, 14.638524536351927]))\n",
    "modelnames.append('LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sig = [[28.65432678, 27.33126691, 32.05417722, 26.68404638, 27.20298043,\n",
    "        34.19808939, 47.19218419, 28.04451459, 36.88454725, 57.25105967],\n",
    "[22.91231799, 20.87124175, 24.88385949, 16.62592638, 17.30676079,\n",
    "        27.38186206, 42.44519322, 21.06660697, 32.39765622, 39.0779806 ],\n",
    "[25.93480661, 22.51155399, 27.13089021, 21.71908598, 19.72367763,\n",
    "        30.67499452, 40.91537458, 21.51829287, 34.05657651, 41.19434126],\n",
    "[25.35984617, 23.7318953 , 25.73767935, 18.88889724, 22.45273873,\n",
    "        29.24297504, 44.59392753, 22.74959161, 34.69550291, 41.52428139],\n",
    "[22.9676217 , 21.06257806, 24.23064445, 16.80321115, 16.97869103,\n",
    "        27.55799536, 42.19653324, 20.72229717, 32.58228819, 39.20836571],\n",
    "[24.28287418, 21.80293   , 22.19905195, 16.61266033, 17.25766192,\n",
    "        26.27257479, 47.25850793, 22.18165508, 33.67434825, 42.93492414],\n",
    "[23.85962672, 20.86479869, 24.39581339, 17.68919607, 18.09929648,\n",
    "        27.95269707, 42.26654614, 21.65914963, 33.22886124, 39.55790887],\n",
    " [24.27781282, 21.74906995, 25.6492858 , 18.71238073, 19.49997929,\n",
    "        28.18585657, 43.48910747, 22.01955238, 32.83176996, 39.08202969],\n",
    "[18.01777016, 19.08547714, 18.61810288, 17.61940315, 17.46961741,\n",
    "        18.34671841, 17.44214568, 19.16014616, 18.43032057, 18.7440025 ],\n",
    "[20.998323330681167, 19.234475326514044, 18.502516721637832, 18.211856287639307, 16.800838330295846,\n",
    " 20.60939168987153, 20.9860453177409, 19.418558315224928, 18.131871812411365, 19.555813843401406],\n",
    "[18.661109434586884, 17.4749167644212, 16.328090963137054, 18.26126620806615, 16.439259683040785,\n",
    " 18.517395004384976, 17.763836526777567, 16.69404935545286, 16.896091157174286, 16.87094464406945],\n",
    "[19.174632851347162, 14.965517455312737, 24.815246362103796, 14.69731472868515, 15.458120581318491,\n",
    " 26.58457925028661, 17.42879313870755, 19.904137526577433, 15.207624142549776, 14.638524536351927],\n",
    "[14.253180583572306, 14.214632431174541, 15.24421802136218, 14.607750474139724, 13.983064409189407,\n",
    " 14.489753037532221, 13.661561807233946, 15.069159143216966, 14.689767829931357, 14.152714450603238]]\n",
    "\n",
    "modelnames = ['SGD',\n",
    " 'ExtraTrees',\n",
    " 'AdaBoost',\n",
    " 'Bagging',\n",
    " 'RandomForest',\n",
    " 'SVR',\n",
    " 'GradientBoosting',\n",
    " 'KNN',\n",
    " 'CNN2D',\n",
    " 'CNN1D',\n",
    " 'CNN',\n",
    " 'LSTM',\n",
    " 'GRU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sig = [[28.65432678, 27.33126691, 32.05417722, 26.68404638, 27.20298043,\n",
    "        34.19808939, 47.19218419, 28.04451459, 36.88454725, 57.25105967],\n",
    "[22.91231799, 20.87124175, 24.88385949, 16.62592638, 17.30676079,\n",
    "        27.38186206, 42.44519322, 21.06660697, 32.39765622, 39.0779806 ],\n",
    "[25.93480661, 22.51155399, 27.13089021, 21.71908598, 19.72367763,\n",
    "        30.67499452, 40.91537458, 21.51829287, 34.05657651, 41.19434126],\n",
    "[25.35984617, 23.7318953 , 25.73767935, 18.88889724, 22.45273873,\n",
    "        29.24297504, 44.59392753, 22.74959161, 34.69550291, 41.52428139],\n",
    "[22.9676217 , 21.06257806, 24.23064445, 16.80321115, 16.97869103,\n",
    "        27.55799536, 42.19653324, 20.72229717, 32.58228819, 39.20836571],\n",
    "[24.28287418, 21.80293   , 22.19905195, 16.61266033, 17.25766192,\n",
    "        26.27257479, 47.25850793, 22.18165508, 33.67434825, 42.93492414],\n",
    "[23.85962672, 20.86479869, 24.39581339, 17.68919607, 18.09929648,\n",
    "        27.95269707, 42.26654614, 21.65914963, 33.22886124, 39.55790887],\n",
    " [24.27781282, 21.74906995, 25.6492858 , 18.71238073, 19.49997929,\n",
    "        28.18585657, 43.48910747, 22.01955238, 32.83176996, 39.08202969],\n",
    "[18.01777016, 19.08547714, 18.61810288, 17.61940315, 17.46961741,\n",
    "        18.34671841, 17.44214568, 19.16014616, 18.43032057, 18.7440025 ],\n",
    "[20.998323330681167, 19.234475326514044, 18.502516721637832, 18.211856287639307, 16.800838330295846,\n",
    " 20.60939168987153, 20.9860453177409, 19.418558315224928, 18.131871812411365, 19.555813843401406],\n",
    "[18.661109434586884, 17.4749167644212, 16.328090963137054, 18.26126620806615, 16.439259683040785,\n",
    " 18.517395004384976, 17.763836526777567, 16.69404935545286, 16.896091157174286, 16.87094464406945],\n",
    "[19.174632851347162, 14.965517455312737, 24.815246362103796, 14.69731472868515, 15.458120581318491,\n",
    " 26.58457925028661, 17.42879313870755, 19.904137526577433, 15.207624142549776, 14.638524536351927],\n",
    "[14.253180583572306, 14.214632431174541, 15.24421802136218, 14.607750474139724, 13.983064409189407,\n",
    " 14.489753037532221, 13.661561807233946, 15.069159143216966, 14.689767829931357, 14.152714450603238]]\n",
    "\n",
    "modelnames = ['SGD',\n",
    " 'ExtraTrees',\n",
    " 'AdaBoost',\n",
    " 'Bagging',\n",
    " 'RF',\n",
    " 'SVR',\n",
    " 'GBR',\n",
    " 'KNN',\n",
    " 'CNN2D',\n",
    " 'CNN1D',\n",
    " 'CNN',\n",
    " 'LSTM',\n",
    " 'GRU']\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mannwhitneyu\n",
    "p = np.zeros((len(results_sig),len(results_sig)))\n",
    "s = np.zeros((len(results_sig),len(results_sig)))\n",
    "\n",
    "for i in range(len(results_sig)):\n",
    "    for j in range(len(results_sig)):\n",
    "        s[i][j], p[i][j] = mannwhitneyu(results_sig[i], results_sig[j])\n",
    "\n",
    "        \n",
    "s = np.around(s, decimals=5)\n",
    "p = np.around(p, decimals=5)\n",
    "from string import ascii_letters\n",
    "\n",
    "\n",
    "sns.set(style=\"white\",font_scale=1.13,rc={\"lines.linewidth\": 3.5, \"font.weight\": 'bold'})\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(p, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "model_inv = modelnames[::-1]\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(p*100, mask=mask,\n",
    "            square=True, linewidths=.5,\n",
    "            annot=True, cbar_kws=dict(shrink=0.5, use_gridspec=False,location=\"top\",pad=0.0),\n",
    "            annot_kws=dict(size=12),\n",
    "            ax=ax, cmap='OrRd')\n",
    "#cbar_kws={\"shrink\": 0.5}\n",
    "plt.xticks(plt.xticks()[0], modelnames)\n",
    "plt.yticks(plt.yticks()[0], modelnames)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_val_loss = [3031.7217674082367,\n",
    " 2924.329375917949,\n",
    " 2529.156398353663,\n",
    " 2457.075932837938,\n",
    " 2903.6382146952105,\n",
    " 2230.677318815201,\n",
    " 1928.446299728082,\n",
    " 1994.0078922626383,\n",
    " 2220.6305168783315,\n",
    " 1676.6130244283179,\n",
    " 1801.8468650117213,\n",
    " 1922.9241259871155,\n",
    " 1633.829518982072,\n",
    " 1715.6588729806497,\n",
    " 1634.930201228784,\n",
    " 1782.76660640656,\n",
    " 1652.0217194113873,\n",
    " 1752.3951839568124,\n",
    " 1768.2913672734821,\n",
    " 1817.443348927833,\n",
    " 2009.4655160374111,\n",
    " 1873.0356189874835,\n",
    " 1769.81899948185,\n",
    " 2240.4773564457623,\n",
    " 1820.0706298681073,\n",
    " 2209.345026809342,\n",
    " 2033.1465486652996,\n",
    " 1921.4363754593055,\n",
    " 1899.8828418573826,\n",
    " 1972.224581357303,\n",
    " 2358.5931931753007,\n",
    " 2020.0059627026928,\n",
    " 2319.431614374087,\n",
    " 2267.0938544932947,\n",
    " 1718.6965160013056,\n",
    " 2066.163143975394,\n",
    " 2273.4551649147693,\n",
    " 2037.6107264019195,\n",
    " 1904.3933727584458,\n",
    " 2208.59751421524,\n",
    " 1939.0497927438646,\n",
    " 1957.672191446871,\n",
    " 2219.176108365697,\n",
    " 2082.099818543242,\n",
    " 1976.6663236542354,\n",
    " 2248.774754681014,\n",
    " 2019.2041749759596,\n",
    " 2153.85292485728,\n",
    " 2262.051716893168,\n",
    " 2471.1514413751443,\n",
    " 2227.6516234642254,\n",
    " 2235.7627665007194,\n",
    " 2179.578035099436,\n",
    " 2476.6357464087523,\n",
    " 2241.5153351630215,\n",
    " 2442.206079392206,\n",
    " 2494.067430554604,\n",
    " 2367.9417443783645,\n",
    " 2313.3071518549846,\n",
    " 2390.2178504667045,\n",
    " 2546.9236205916286,\n",
    " 2622.942600224294,\n",
    " 3448.0582908405468,\n",
    " 2636.9137494569463,\n",
    " 2429.2870739355108,\n",
    " 2674.043030970221,\n",
    " 3006.8156724830333,\n",
    " 2983.0378965780037,\n",
    " 2688.577234873696,\n",
    " 2609.518538888079,\n",
    " 3061.8731124514625,\n",
    " 2785.98852979353,\n",
    " 2962.6271328136763,\n",
    " 2439.2875856499013,\n",
    " 2876.2468739810174,\n",
    " 3362.6285262464667,\n",
    " 2896.4771418495784,\n",
    " 2992.5780754175858,\n",
    " 2899.7267058149755,\n",
    " 2694.272812261603,\n",
    " 2866.649811448424,\n",
    " 2821.8350234821,\n",
    " 2969.1428545954004,\n",
    " 3399.523882686416,\n",
    " 3015.7862185350623,\n",
    " 3166.9297749525836,\n",
    " 2940.424503239915,\n",
    " 3038.812027133241,\n",
    " 3124.7072823626113,\n",
    " 3137.3042251517713,\n",
    " 2940.269537949508,\n",
    " 3165.4724820368415,\n",
    " 3023.844465952071,\n",
    " 3112.7998968197917,\n",
    " 3193.2929540448176,\n",
    " 3295.7612485993745,\n",
    " 3092.3861556928987,\n",
    " 3166.823697520492,\n",
    " 3183.712142377754,\n",
    " 3105.19446992766]\n",
    "\n",
    "overfit_loss = [2117.854875908492,\n",
    " 1318.594980423667,\n",
    " 1242.2128267009925,\n",
    " 1223.8522811969467,\n",
    " 1147.1509638561706,\n",
    " 1103.7239519363166,\n",
    " 1072.40451032687,\n",
    " 1050.7910454059404,\n",
    " 1008.9502477388027,\n",
    " 1006.4736362591187,\n",
    " 981.9634689799387,\n",
    " 940.719219886461,\n",
    " 947.4961189072396,\n",
    " 897.4693248813933,\n",
    " 895.0156377824935,\n",
    " 860.535063699054,\n",
    " 890.0899875655534,\n",
    " 854.7538881744848,\n",
    " 867.5033236021745,\n",
    " 855.3406971651191,\n",
    " 847.0040216809554,\n",
    " 839.1765548197333,\n",
    " 834.4820997738587,\n",
    " 831.8846443778755,\n",
    " 828.5163069766088,\n",
    " 831.0652472071156,\n",
    " 835.5238137584466,\n",
    " 795.750401384913,\n",
    " 796.1582904752444,\n",
    " 808.1163971818537,\n",
    " 788.8935312958831,\n",
    " 787.8450201559497,\n",
    " 788.0843459757002,\n",
    " 788.2548550087495,\n",
    " 829.7563299384753,\n",
    " 780.7672449992947,\n",
    " 778.7846778046921,\n",
    " 770.6499253468529,\n",
    " 807.2972669759515,\n",
    " 782.1444923062852,\n",
    " 778.896525807512,\n",
    " 773.1118810957454,\n",
    " 738.4298138401192,\n",
    " 756.3484916915407,\n",
    " 729.5380932384849,\n",
    " 736.7773662030287,\n",
    " 744.1571496405853,\n",
    " 720.4840751364738,\n",
    " 726.4776419340689,\n",
    " 705.5301465434599,\n",
    " 705.4494199694507,\n",
    " 690.8033095910538,\n",
    " 672.7098932706799,\n",
    " 694.9502425650278,\n",
    " 706.7397712817631,\n",
    " 666.8579170456409,\n",
    " 677.7280229277698,\n",
    " 650.6826516306077,\n",
    " 632.5243918864317,\n",
    " 667.3959013559706,\n",
    " 666.7098953324966,\n",
    " 620.7452211822973,\n",
    " 614.494432064313,\n",
    " 631.6037904793834,\n",
    " 625.8404595599911,\n",
    " 630.3150801211154,\n",
    " 596.4697085331449,\n",
    " 581.6061258617162,\n",
    " 574.7508498127102,\n",
    " 575.2277907176002,\n",
    " 576.5832696401262,\n",
    " 573.7285713036645,\n",
    " 540.4749221787333,\n",
    " 594.2843494796344,\n",
    " 531.4948174355274,\n",
    " 534.8225302568507,\n",
    " 511.78813137439835,\n",
    " 525.400460178613,\n",
    " 515.8699818654,\n",
    " 518.6337601855278,\n",
    " 514.9518282577718,\n",
    " 515.6517586864641,\n",
    " 512.1800023963233,\n",
    " 489.89661498781095,\n",
    " 508.75588046053656,\n",
    " 482.40475688512873,\n",
    " 460.3336007174783,\n",
    " 457.1335752432728,\n",
    " 474.649447455466,\n",
    " 466.7470601649773,\n",
    " 450.12463765689193,\n",
    " 448.7378544251169,\n",
    " 441.0108043458847,\n",
    " 443.4844607538752,\n",
    " 424.4346864704765,\n",
    " 433.9024677821156,\n",
    " 418.8324143843742,\n",
    " 442.00669943062474,\n",
    " 411.06947403689776,\n",
    " 408.27150801857397]\n",
    "\n",
    "normal_val_loss = [2638.592434076495,\n",
    " 2872.791335938469,\n",
    " 2576.4507605451035,\n",
    " 2625.184218183937,\n",
    " 3042.2874636574397,\n",
    " 2442.5371465488356,\n",
    " 2522.1581174024377,\n",
    " 2681.4429111653717,\n",
    " 2877.437149636059,\n",
    " 1982.682385399228,\n",
    " 3060.2699389295512,\n",
    " 2795.375446181178,\n",
    " 3158.119543495092,\n",
    " 2743.863231131279,\n",
    " 2974.575469624699,\n",
    " 2709.1055830784667,\n",
    " 2431.0303830084076,\n",
    " 2262.3382767830844,\n",
    " 2578.5001589803196,\n",
    " 2754.7460016004084,\n",
    " 3136.859801874139,\n",
    " 2092.9638773773263,\n",
    " 3088.8971823521483,\n",
    " 3096.4523758228675,\n",
    " 2697.2636127255673,\n",
    " 2820.7369060386604,\n",
    " 2716.984460869614,\n",
    " 2504.8531265604793,\n",
    " 2244.073823440102,\n",
    " 2335.029008662079,\n",
    " 3083.178676458173,\n",
    " 2683.7842058627243,\n",
    " 3116.9801504087554,\n",
    " 2665.9886563186474,\n",
    " 2991.282171902473,\n",
    " 2598.231815995543,\n",
    " 2553.607684250051,\n",
    " 2494.7713689133693,\n",
    " 2148.32272644216,\n",
    " 2695.826736086891,\n",
    " 3214.99888620355,\n",
    " 3069.9916465655474,\n",
    " 3046.0417006442876,\n",
    " 2770.57907898572,\n",
    " 2790.1888633260924,\n",
    " 2907.615921297311,\n",
    " 3123.0926930953046,\n",
    " 2668.556151392238,\n",
    " 3224.2357899845324,\n",
    " 3338.3759276720943,\n",
    " 2511.3504787972724,\n",
    " 2254.5436211583838,\n",
    " 2667.3054845554757,\n",
    " 2544.772331237793,\n",
    " 3506.1539164605865,\n",
    " 2524.099858030981,\n",
    " 2888.4855573139494,\n",
    " 2311.832234475618,\n",
    " 3083.284755680837,\n",
    " 2145.8216943373213,\n",
    " 2371.269276685996,\n",
    " 2305.823315955614,\n",
    " 2747.6176979168745,\n",
    " 2544.333734698306,\n",
    " 2816.2784413880504,\n",
    " 2188.012582220999,\n",
    " 2524.493170653882,\n",
    " 2889.3734792419573,\n",
    " 2396.0392248452117,\n",
    " 2616.818992286042,\n",
    " 2648.8355990559066,\n",
    " 2514.9047139053173,\n",
    " 2893.865869863774,\n",
    " 2907.057890712539,\n",
    " 2731.25567453951,\n",
    " 2524.621893417808,\n",
    " 2140.6801705738853,\n",
    " 2505.2862458348004,\n",
    " 2396.5942409022323,\n",
    " 2580.6899403855373,\n",
    " 2239.7707187367137,\n",
    " 2548.967884963332,\n",
    " 2545.521503336035,\n",
    " 3018.5936791902227,\n",
    " 3127.8285597370864,\n",
    " 2643.5994356462475,\n",
    " 2723.715581033235,\n",
    " 2944.354026431129,\n",
    " 2729.128236454901,\n",
    " 2474.6189677785583,\n",
    " 2879.9978223874186,\n",
    " 2892.010249935851,\n",
    " 2872.3816638263174,\n",
    " 2785.5337157390013,\n",
    " 2812.6046111178234,\n",
    " 2594.704290480841,\n",
    " 2595.821296838946,\n",
    " 2746.867856170585,\n",
    " 2747.966006151403,\n",
    " 2771.721996956131]\n",
    "\n",
    "normal_loss = [2589.9523749526707,\n",
    " 1357.6303935121587,\n",
    " 1295.7775770276212,\n",
    " 1263.5625174408142,\n",
    " 1223.2883251236105,\n",
    " 1185.4622356366472,\n",
    " 1144.3526121338953,\n",
    " 1117.6809310215876,\n",
    " 1094.793273575734,\n",
    " 1062.1133095615114,\n",
    " 1050.5481429479596,\n",
    " 1031.60639882546,\n",
    " 1025.5080043758826,\n",
    " 998.800037426587,\n",
    " 980.7274081525336,\n",
    " 973.193023883591,\n",
    " 986.3515402046849,\n",
    " 966.5710233497343,\n",
    " 971.2183342593044,\n",
    " 958.2290499547693,\n",
    " 954.4207491327445,\n",
    " 941.1572293782644,\n",
    " 946.586421796097,\n",
    " 950.9569667989983,\n",
    " 937.7930805689297,\n",
    " 938.9171452619947,\n",
    " 925.8643353383422,\n",
    " 921.8691014427916,\n",
    " 919.5663294934923,\n",
    " 917.376164232046,\n",
    " 914.4926228241064,\n",
    " 896.6952513956037,\n",
    " 906.2257368193913,\n",
    " 895.5873120431075,\n",
    " 894.234665496878,\n",
    " 888.9109127443529,\n",
    " 899.2524265350132,\n",
    " 876.7222555882615,\n",
    " 885.1137196976048,\n",
    " 889.7987210259378,\n",
    " 875.8393855728108,\n",
    " 881.5041409701234,\n",
    " 867.3140119116375,\n",
    " 870.8339095879887,\n",
    " 855.6559508019482,\n",
    " 864.8111066582028,\n",
    " 841.153224999643,\n",
    " 838.9718882079355,\n",
    " 823.4259683637017,\n",
    " 819.1815524515677,\n",
    " 802.833929607997,\n",
    " 789.152630134785,\n",
    " 776.8046310731577,\n",
    " 789.2295117439842,\n",
    " 771.0726751148524,\n",
    " 750.0201712594093,\n",
    " 760.453717503001,\n",
    " 749.8950163038916,\n",
    " 734.3403366395639,\n",
    " 733.0620164655442,\n",
    " 708.5132822916789,\n",
    " 720.9310152598738,\n",
    " 707.4754399870432,\n",
    " 709.2067429880387,\n",
    " 695.5209502106377,\n",
    " 708.4961062939998,\n",
    " 684.8256047408056,\n",
    " 672.8181892982118,\n",
    " 677.9770122837493,\n",
    " 673.9493581149532,\n",
    " 655.5554489450208,\n",
    " 649.7897549357119,\n",
    " 651.4393991638749,\n",
    " 650.3464324618435,\n",
    " 633.510986093478,\n",
    " 631.3506648943045,\n",
    " 616.1067069093259,\n",
    " 629.2077481255471,\n",
    " 619.4336482496593,\n",
    " 606.3929168322275,\n",
    " 603.2005095875855,\n",
    " 583.479366809049,\n",
    " 587.8350927398945,\n",
    " 591.6567520831118,\n",
    " 589.8380269087683,\n",
    " 566.2965028113331,\n",
    " 558.5184320825,\n",
    " 554.8317355562671,\n",
    " 548.3461182117612,\n",
    " 545.3387662161874,\n",
    " 537.7250181609113,\n",
    " 542.8820531918059,\n",
    " 534.5345085222419,\n",
    " 531.1407003428554,\n",
    " 526.56610185894,\n",
    " 525.9916142996027,\n",
    " 519.320765918855,\n",
    " 507.53622640974095,\n",
    " 508.82544293051717,\n",
    " 519.6103655797755]\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df_normal = pd.DataFrame({'Loss':normal_loss,'Overfit Loss':overfit_loss,\n",
    "                          'Validation Loss': normal_val_loss, 'Overfit Validation Loss': overfit_val_loss})\n",
    "df_normal = df_normal.rolling(10).sum()\n",
    "df_overfit = pd.DataFrame({'Loss':overfit_loss, 'Validation Loss': overfit_val_loss})\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "sns.set(style=\"white\",font_scale=1.44,rc={\"lines.linewidth\": 3.5, \"font.weight\": 'bold'})\n",
    "palette = sns.color_palette(\"husl\", 4)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "ax = sns.lineplot(data=df_normal, palette=palette)\n",
    "#ax = sns.lineplot(data=df_normal)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
